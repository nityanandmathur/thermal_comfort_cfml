{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29415f20-1f94-4ebb-8ef6-e565d81f90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "pd.options.display.width = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76765057-8667-4c44-84b3-e7c7d6de1968",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## 1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc8743-ab71-4588-a734-d2d804c83da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data = pd.read_csv(\n",
    "    'IndividualClothingValue.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7755964-b8ea-423a-a81d-45b77b022f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677f554-984b-4420-8c91-5fa27aa9538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ac98f-93cd-47f6-9aaf-be750d9f700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf21588-7c32-4abc-8851-fd556fb76d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349cea7-b27d-4902-880d-2365ae033cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b2226-42a9-4ebb-b352-3a43bfcf1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data = winter_data.astype({\n",
    "    'DAY':'category',\n",
    "    'School':'category',\n",
    "    'SchoolType': 'category',\n",
    "    'StartTime': 'category',\n",
    "    'Grade': 'category',\n",
    "    'Gender': 'category',\n",
    "    'FormalClothing': 'bool',\n",
    "    'Pant': 'bool',\n",
    "    'Trackpant': 'bool',\n",
    "    'Halfshirt': 'bool',\n",
    "    'Blazer': 'bool',\n",
    "    'Jacket': 'bool',\n",
    "    'Skirt': 'bool',\n",
    "    'FullShirt': 'bool',\n",
    "    'HalfSweater': 'bool',\n",
    "    'Tshirt': 'bool',\n",
    "    'Socks': 'bool',\n",
    "    'Thermal': 'bool',\n",
    "    'Vest': 'bool',\n",
    "    'FullSweater': 'bool',\n",
    "    'TSV':'category',\n",
    "    'TPV':'category',\n",
    "    'TCV':'category',\n",
    "    'TSL':'category',\n",
    "    'MC':'category',\n",
    "    'SwC':'category',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d655e-f4f7-48d7-9181-cb16f70779df",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd1dc6-893a-422d-84f4-5bf8e94ec32c",
   "metadata": {},
   "source": [
    "## Distributions of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3655b57-2a9f-4041-8314-c5f78676161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with columns containing only the numerical features\n",
    "num_features = winter_data.select_dtypes(exclude=['bool', 'category']).copy()\n",
    "\n",
    "# we plot individual column distributions with null entry rows dropped\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 6))\n",
    "for i in range(len(num_features.columns)):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    sns.distplot(num_features.iloc[:,i].dropna())\n",
    "    plt.xlabel(num_features.columns[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c308279-7d9b-40fe-8cac-20e51eb28abf",
   "metadata": {},
   "source": [
    "## Distributions of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfb75d-379b-432c-b6c9-0707943929ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with columns containing only the categorical features\n",
    "cat_features = winter_data.select_dtypes(include=['category']).copy()\n",
    "\n",
    "# we plot individual column distributions with null entry rows dropped\n",
    "fig, axs = plt.subplots(nrows=7, ncols=2, figsize=(10, 30))\n",
    "plt.subplots_adjust(right=1.5, top=1.25)\n",
    "\n",
    "for i in range(len(cat_features.columns)):\n",
    "    plt.subplot(7, 2, i+1)\n",
    "    sns.countplot(y=cat_features.columns[i], data=cat_features)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd4976-ecec-416e-b892-96269f70b385",
   "metadata": {},
   "source": [
    "## Distributions of Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616899da-9afd-40d6-9478-2a251a8b7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with columns containing only the categorical features\n",
    "bool_features = winter_data.select_dtypes(include='bool').copy()\n",
    "\n",
    "# we plot individual column distributions with null entry rows dropped\n",
    "fig, axs = plt.subplots(nrows=7, ncols=2, figsize=(10, 30))\n",
    "plt.subplots_adjust(right=1.5, top=1.25)\n",
    "\n",
    "for i in range(len(bool_features.columns)):\n",
    "    plt.subplot(7, 2, i+1)\n",
    "    sns.countplot(y=bool_features.columns[i], data=bool_features)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445a69c-e0a0-4e24-a90f-3d69860a4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsl_target_data = winter_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c77b9-9f55-4021-a6ad-ad5ccc286bcc",
   "metadata": {},
   "source": [
    "# Machine Learning Models (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4873a70-0c16-4d11-8ab0-536c1ade233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import  make_pipeline\n",
    "\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, StratifiedKFold\n",
    "\n",
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler\n",
    "\n",
    "# For dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d2bb8-13a8-428f-bc40-0986c54c90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples where target has samples < 2.\n",
    "clean_winter_data = winter_data.copy()[winter_data.TPV != -1][winter_data.TSV != 2][winter_data.TCV != -3]\n",
    "clean_winter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d716038-ad3d-491a-a349-08f3069e6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_data = pd.get_dummies(clean_winter_data.copy().drop(['TSV', 'TPV',\n",
    "       'TSL','TCV'], axis=1), cat_features.drop(['TSV', 'TPV',\n",
    "       'TSL','TCV'], axis=1).columns)\n",
    "sklearn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aede67-bf5c-4e3e-8876-b0a72de0ff4f",
   "metadata": {},
   "source": [
    "## **1. Split the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4ca8d-2109-46b0-9ea4-0bf9bc32e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our features dataframe X and labels y\n",
    "X= sklearn_data\n",
    "y= clean_winter_data.TSV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=505, stratify=y)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce338b0-a57d-41e5-a0b7-b2c231961b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline dictionary\n",
    "pipelines = {\n",
    "    'lr': make_pipeline(StandardScaler(), \n",
    "                        LogisticRegressionCV(n_jobs=-1, random_state=505)),\n",
    "   # 'rdg': make_pipeline(StandardScaler(),\n",
    "    #                       RidgeClassifierCV()),\n",
    "    'svc': make_pipeline(StandardScaler(),\n",
    "                        SVC(random_state=505, probability=True)),\n",
    "    'rf': make_pipeline(StandardScaler(), \n",
    "                            RandomForestClassifier(n_estimators=100, max_depth=15, random_state=505)),\n",
    "    #Kneighbours\n",
    "    'knc': make_pipeline(StandardScaler(),\n",
    "                            KNeighborsClassifier(n_jobs=-1)),\n",
    "    #gausian process\n",
    "    'gpc': make_pipeline(StandardScaler(),\n",
    "                            GaussianProcessClassifier(n_jobs=-1,random_state=505)),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222dff5d-7703-4e57-b76a-db98c6e532e0",
   "metadata": {},
   "source": [
    "## Choice of metric evaluation\n",
    "\n",
    "1. Quadratic Cohen's Kappa\n",
    "\n",
    "    Cohen’s kappa statistic measures interrater reliability (sometimes called interobserver agreement). Interrater reliability, or precision, happens when your data raters (or collectors) give the same score to the same data item. In our case, we have labels VS predictions. The Kappa statistic takes into account this element of chance.\n",
    "\n",
    "The Kappa statistic varies from 0 to 1, where.\n",
    "\n",
    "0 = agreement equivalent to chance.\\ 0.1 – 0.20 = slight agreement.\\ 0.21 – 0.40 = fair agreement.\\ 0.41 – 0.60 = moderate agreement.\\ 0.61 – 0.80 = substantial agreement.\\ 0.81 – 0.99 = near perfect agreement\\ 1 = perfect agreement.\n",
    "\n",
    "2. Micro-averaged Precision score\n",
    "\n",
    "    The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "For multiclass classification, Micro-average is preferable if there is a class imbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af4df5-a292-45e7-bb5f-501fbbbd9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics \n",
    "from sklearn.metrics import precision_score, accuracy_score, cohen_kappa_score\n",
    "\n",
    "# Initiate empty dictionary of fitted base models\n",
    "fitted_basemodels = {}\n",
    "# Loop through model pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "  \n",
    "    # Fit model on X_train, y_train  and predict with X_test\n",
    "    base_mod = pipeline.fit(X_train, y_train)\n",
    "    #pred = pipeline.predict(X_test)\n",
    "    pred = pipeline.predict(X_test)\n",
    "    pred_prob = pipeline.predict_proba(X_test)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_basemodels[name] = base_mod\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted.')\n",
    "   # print('accuracy on test: ', pipeline.score(X_test,y_test))\n",
    "    print('precision scores: ', precision_score(y_test, pred,  average='micro'))\n",
    "    print('Accuracy scores ', accuracy_score(y_test, pred))\n",
    "    print('Kappa on test: ', cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
    "    print('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc427c4-78b0-43d5-a470-7eda07de3789",
   "metadata": {},
   "source": [
    "**3. Hyperparameter tuning (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa6914-00a1-4ac4-96a1-40fc1ff8028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines['rf'].get_params() # This is how we get the format of hyperparameter dictionaries for GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d77e77-7365-4342-8dc4-cc2b47e9fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each models hyperparameters\n",
    "\n",
    "svc_hyparams = {'svc__C': [0.1, 1, 10, 100, 1000],\n",
    "               'svc__gamma': [0.1, 1, 10, 100]\n",
    "               }\n",
    "\n",
    "rf_hyparams  = {\n",
    "    'randomforestclassifier__n_estimators': [100,200],\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33],\n",
    "    'randomforestclassifier__max_depth': [8, 15],\n",
    "    'randomforestclassifier__min_samples_split': [2, 5 ],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 2, 10]\n",
    "}\n",
    "\n",
    "knc_hyparams =  {\n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "    'kneighborsclassifier__p': [1, 1.5 , 2, 10],\n",
    "    'kneighborsclassifier__n_neighbors': np.arange(2,10),\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476bdd5-666a-4be4-b4b8-f4c137621f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a dictionary of hyperparameter dictionaries !?\n",
    "hyperparameters = {\n",
    "    'lr': {},\n",
    "    'svc': svc_hyparams,\n",
    "    'rf': rf_hyparams,\n",
    "    'knc': knc_hyparams,\n",
    "    'gpc':{}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2aefc-11d4-4f61-88be-64cc735a33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv = 5, n_jobs=-1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ac8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for name, model in fitted_models.items():\n",
    "    pickle.dump(model, open(name+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec037da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9d668-defc-4834-9fa0-3899b07bfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    print('precision score: ', precision_score(y_test, pred,  average='micro'))\n",
    "    print('Accuracy scores ', accuracy_score(y_test, pred))\n",
    "    print(name,'Kappa on test:', cohen_kappa_score(y_test, pred, weights='quadratic'))\n",
    "    print('-'*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed43043-171d-435d-acf2-1a43359cd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_names = np.r_[X_train.columns.to_list()]\n",
    "tree_feature_importances = (\n",
    "    fitted_models['rf'].estimator['randomforestclassifier'].feature_importances_.reshape(1,len(feature_names))[0])\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "print(feature_names[sorted_idx])\n",
    "print(tree_feature_importances[sorted_idx])\n",
    "\n",
    "y_ticks = np.arange(0, len(feature_names))\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticklabels(feature_names[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6257c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dice_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0995ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dice_ml.Data(dataframe=X_train, continuous_features=['AvgMaxDailyTemp', 'AvgMinDailyTemp',\n",
    "                                                        'AvgIndoorRelativeHumidity', 'IndoorTempDuringSurvey'],\n",
    "                                                        outcome_name='Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dice_ml.Model(model=fitted_models['lr'], backend='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = dice_ml.Dice(d,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adda766",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance = X_test[0:1].drop('Age', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_exp = exp.generate_counterfactuals(query_instance, total_CFs=4, desired_class=\"opposite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc99d5-c71f-4d46-ad6c-de1dbf87656d",
   "metadata": {},
   "source": [
    "## Machine Learning Models (CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65309bc6-bd36-4a4e-9465-c188fa18a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat\n",
    "# Classification metrics \n",
    "from sklearn.metrics import precision_score, cohen_kappa_score, f1_score, accuracy_score, recall_score\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697e408-b43d-4b06-8eb5-45837788e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_trainer(X: pd.DataFrame,\n",
    "                y,\n",
    "            n_folds = None,\n",
    "            params: dict = None,\n",
    "            del_cols: list = None,\n",
    "            cat_features=None):\n",
    "    \n",
    "    \"\"\"This function trains multiple Catboost model while performing stratified CV with shuffling.\n",
    "    out-of-fold (oof) predictions are evaluated at each fold and printed out \n",
    "    at the end of the routine as a list. The mean scores on all fold is also printed out.\n",
    "    \n",
    "    The metrics used for evaluation are precision_auc and the cohen's kappa\n",
    "    \n",
    "    Output: \n",
    "    models -- a list of models trained on each fold during CV\n",
    "    oof_pred -- prediction array consisting of predictions coming from different models\n",
    "    \"\"\"\n",
    "        \n",
    "    # collect models and scores from each fold\n",
    "    models = []\n",
    "    f1_scores = []\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    scores = []\n",
    "    # CV splitter\n",
    "    folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=505) #GroupKFold(n_splits=n_folds) \n",
    "    #features to use\n",
    "    columns = [col for col in X.columns.values if not col in del_cols]\n",
    "    # dimension of the target \n",
    "    n_target=1\n",
    "    \n",
    "    # collect out-of-sample predictions\n",
    "    data_X, data_y, oof_pred = pd.DataFrame(), pd.Series(), np.zeros((len(X),n_target))\n",
    "\n",
    "\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "     \n",
    "        print('Fold {} started at {}'.format(fold_n + 1,time.ctime()))\n",
    "        #print((train_index, valid_index))\n",
    "        X_train, X_valid = X.iloc[train_index][columns], X.iloc[valid_index][columns]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "        data_X = data_X.append(X_valid)\n",
    "        data_y = data_y.append(y_valid)\n",
    "        print(data_X.shape)\n",
    "        \n",
    "        #Eval set preparation\n",
    "        eval_set = [(X_train, y_train)]\n",
    "       # eval_names = ['train']\n",
    "        eval_set.append((X_valid, y_valid))\n",
    "       # eval_names.append('valid')\n",
    "    \n",
    "        model = cat.CatBoostClassifier()\n",
    "        model.fit(X=X_train, y=y_train, \n",
    "                       eval_set=eval_set, \n",
    "                       verbose=500, early_stopping_rounds=150,\n",
    "                      cat_features=cat_features, use_best_model=True, plot=True)\n",
    "        \n",
    "        oof_pred[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n",
    "        models.append(model)\n",
    "    \n",
    "        print('-'*30)\n",
    "        \n",
    "        scores.append(cohen_kappa_score(y_valid, oof_pred[valid_index], weights='quadratic'))\n",
    "        f1_scores.append(f1_score(y_valid, oof_pred[valid_index],   average=None))\n",
    "        accuracy_scores.append(accuracy_score(y_valid, oof_pred[valid_index]))\n",
    "        precision_scores.append(precision_score(y_valid, oof_pred[valid_index],   average=None))\n",
    "        recall_scores.append(recall_score(y_valid, oof_pred[valid_index],   average=None))\n",
    "\n",
    "    print(f'catb fold kappa scores: {scores}')\n",
    "    print(f'catb fold kappa scores mean: {np.mean(scores)}')\n",
    "    print(f'catb fold f1 scores: {f1_scores}')\n",
    "    print(f'catb fold f1 scores mean: {np.mean(f1_scores)}')\n",
    "    print(f'catb fold precision-micro scores: {precision_scores}')\n",
    "    print(f'catb fold precision-micro scores mean: {np.mean(precision_scores)}')\n",
    "    print(f'catb fold recall-micro scores: {recall_scores}')\n",
    "    print(f'catb fold recall-micro scores mean: {np.mean(recall_scores)}')\n",
    "    print(f'catb fold accuracy scores: {accuracy_scores}')\n",
    "    print(f'catb fold accuracy scores mean: {np.mean(accuracy_scores)}')\n",
    "   \n",
    "    return models, oof_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb45f11-b623-4e49-9718-f666cf2fb33b",
   "metadata": {},
   "source": [
    "# Target: TSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c095fc-cc3a-4ac8-8400-cde83186276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection for Catboost\n",
    "models, oof_pred = oof_trainer(X=tsl_target_data,\n",
    "                    y=tsl_target_data.TSL,\n",
    "                    n_folds = 5,\n",
    "                   # params=params,\n",
    "                    del_cols=['TSV',\n",
    "       'TPV','TCV', 'TSL'],\n",
    "                    cat_features=cat_features.drop(columns=['TSV',\n",
    "       'TPV','TCV', 'TSL']).columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10cb5a-5b26-4306-ad17-dc1d6cd285a7",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f69949-6906-4982-99e7-6070b849b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_model = models[1]\n",
    "best_gb_model.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8259b1e-582b-496a-bbd1-c52ceee40ebb",
   "metadata": {},
   "source": [
    "# Target: TPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f17f9a-c3ed-452d-915a-ce83c4b31577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection for Catboost\n",
    "# Remove all rows with TPV value -1\n",
    "models, oof_pred = oof_trainer(X=tsl_target_data[tsl_target_data.TPV != -1],\n",
    "                    y=tsl_target_data[tsl_target_data.TPV != -1].TPV,\n",
    "                    n_folds = 5,\n",
    "                   # params=params,\n",
    "                    del_cols=['TSV',\n",
    "       'TSL','TCV', 'TSL', 'TPV'],\n",
    "                    cat_features=cat_features.drop(columns=['TSV', 'TPV',\n",
    "       'TSL','TCV', 'TSL']).columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c989c89-d9fe-4ab1-acea-6b9a20d27830",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_model = models[4]\n",
    "best_gb_model.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20020847-d228-4b9f-9f90-bc37c47eeea2",
   "metadata": {},
   "source": [
    "# Target: TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96787d-fa72-4e99-89d6-d1520cddc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, oof_pred = oof_trainer(X=tsl_target_data[tsl_target_data.TSV != 2],\n",
    "                    y=tsl_target_data[tsl_target_data.TSV != 2].TSV,\n",
    "                    n_folds = 5,\n",
    "                   # params=params,\n",
    "                    del_cols=['TSV',\n",
    "       'TSL','TCV', 'TSL', 'TPV'],\n",
    "                    cat_features=cat_features.drop(columns=['TSV', 'TPV',\n",
    "       'TSL','TCV', 'TSL']).columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6f5f8-f542-4de6-8038-6ee26caa7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_model = models[0]\n",
    "best_gb_model.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db6a5d-638f-40b0-898b-51628cc40b1b",
   "metadata": {},
   "source": [
    "# Target: TCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e3289-a1d8-42d0-acec-65910b7d28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, oof_pred = oof_trainer(X=tsl_target_data[tsl_target_data.TCV != -3],\n",
    "                    y=tsl_target_data[tsl_target_data.TCV != -3].TCV,\n",
    "                    n_folds = 5,\n",
    "                   # params=params,\n",
    "                    del_cols=['TSV',\n",
    "       'TSL','TCV', 'TSL', 'TPV'],\n",
    "                    cat_features=cat_features.drop(columns=['TSV', 'TPV',\n",
    "       'TSL','TCV', 'TSL']).columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44302a28-73ad-481f-bb2c-157ee3b5632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_model = models[0]\n",
    "best_gb_model.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e868f-5092-4bb7-9a00-6cc13baf7db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
